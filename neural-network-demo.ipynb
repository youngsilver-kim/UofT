{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngsilver-kim/UofT/blob/main/neural-network-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network\n",
        "Neural network is a supervised machine learning algorithm used to predict a target variable (y) based on independent variables (X). They consist of interconnected nodes called neurons that learn and represent patterns within the data. By adjusting weights and biases, the network minimizes the difference between predicted and actual values during training. When making predictions, input data is processed through the network, producing a summarized prediction that captures complex relationships and nonlinearities in the data.\n",
        "\n",
        "In this demo, we will implement a simple multi-layer sigmoid neuron network to classify hand written digits. For historical reasons, this is sometimes also called multilayer perceptrons (MLP), despite being made up of sigmoid neurons, not perceptrons"
      ],
      "metadata": {
        "id": "e47eKiuZehcm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries and initialize random generator"
      ],
      "metadata": {
        "id": "CKQNF4rHehcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "np.set_printoptions(threshold=10) # printing format"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:43.540504Z",
          "iopub.execute_input": "2023-07-11T14:05:43.541870Z",
          "iopub.status.idle": "2023-07-11T14:05:43.581686Z",
          "shell.execute_reply.started": "2023-07-11T14:05:43.541817Z",
          "shell.execute_reply": "2023-07-11T14:05:43.579943Z"
        },
        "trusted": true,
        "id": "vRQa2F1Kehcr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load the MNIST image data\n",
        "Load the MNIST data as a tuple containing the training data,\n",
        "the validation data, and the test data."
      ],
      "metadata": {
        "_uuid": "22315cef-88f4-4bea-9cef-5f51f500e782",
        "_cell_guid": "a3dedc54-92ac-4c42-bfa2-f9dde81c070b",
        "trusted": true,
        "id": "0hdAvIcbehcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBgrPU_-hyA7",
        "outputId": "ab36924f-9aa7-420c-a31f-9a8e6da13fe2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-17 14:15:18--  https://raw.githubusercontent.com/mnielsen/neural-networks-and-deep-learning/master/data/mnist.pkl.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17051982 (16M) [application/octet-stream]\n",
            "Saving to: ‘mnist.pkl.gz’\n",
            "\n",
            "mnist.pkl.gz        100%[===================>]  16.26M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2025-07-17 14:15:19 (183 MB/s) - ‘mnist.pkl.gz’ saved [17051982/17051982]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!yes | gzip -d mnist.pkl.gz"
      ],
      "metadata": {
        "id": "9tLXDVR5h_ts"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('mnist.pkl', 'rb') as file:\n",
        "    data = pickle._Unpickler(file)\n",
        "    data.encoding = 'latin1'\n",
        "    training_data, validation_data, test_data = data.load()"
      ],
      "metadata": {
        "_uuid": "232aee20-c920-459f-8e91-704065550e30",
        "_cell_guid": "1486fe75-3196-4f2e-9ec1-d259ac435a2f",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:43.584410Z",
          "iopub.execute_input": "2023-07-11T14:05:43.585017Z",
          "iopub.status.idle": "2023-07-11T14:05:46.217254Z",
          "shell.execute_reply.started": "2023-07-11T14:05:43.584978Z",
          "shell.execute_reply": "2023-07-11T14:05:46.216187Z"
        },
        "trusted": true,
        "id": "4--EvRBPehcs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``training_data`` is returned as a tuple with two entries.\n",
        "The first entry contains the actual training images.  "
      ],
      "metadata": {
        "id": "nsZOjp2Uehcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_inputs, training_results = training_data\n",
        "\"\"\"\n",
        "#same as:\n",
        "training_inputs =training_data[0]\n",
        "training_results =training_data[1]\n",
        "\"\"\""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.218745Z",
          "iopub.execute_input": "2023-07-11T14:05:46.219479Z",
          "iopub.status.idle": "2023-07-11T14:05:46.228687Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.219432Z",
          "shell.execute_reply": "2023-07-11T14:05:46.227551Z"
        },
        "trusted": true,
        "id": "nPysA1oHehcu",
        "outputId": "d5ab2a54-6212-4f6b-d1cc-eaad8d4476fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#same as:\\ntraining_inputs =training_data[0]\\ntraining_results =training_data[1]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a\n",
        "numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
        "numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
        "pixels in a single MNIST image.\n",
        "\n",
        "One example image:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork1.png\" alt=\"Image Description\" width=\"30%\">\n"
      ],
      "metadata": {
        "id": "lJF5LFqaehcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(training_inputs)\n",
        "display(training_inputs.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.232759Z",
          "iopub.execute_input": "2023-07-11T14:05:46.233869Z",
          "iopub.status.idle": "2023-07-11T14:05:46.250859Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.233812Z",
          "shell.execute_reply": "2023-07-11T14:05:46.249298Z"
        },
        "trusted": true,
        "id": "ST4D2GIRehcw",
        "outputId": "16b1785b-bdab-4ca5-a207-0c295caeefc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(50000, 784)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second entry in the ``training_data`` tuple is a numpy ndarray\n",
        "containing 50,000 entries.  Those entries are just the digit\n",
        "values (0...9) for the corresponding images contained in the first\n",
        "entry of the tuple.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZqtnNrnLehcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(training_results)\n",
        "display(training_results.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.252530Z",
          "iopub.execute_input": "2023-07-11T14:05:46.252935Z",
          "iopub.status.idle": "2023-07-11T14:05:46.269564Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.252896Z",
          "shell.execute_reply": "2023-07-11T14:05:46.268127Z"
        },
        "trusted": true,
        "id": "-jk23abmehcx",
        "outputId": "455e384c-f718-4ab5-9d00-ce2d45b1a6d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 8, 4, 8])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(50000,)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``validation_data`` and ``test_data`` are the same, except\n",
        "each contains only 10,000 images."
      ],
      "metadata": {
        "id": "6xCHVIZoehcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(validation_data[0].shape,validation_data[1].shape)\n",
        "print(test_data[0].shape,test_data[1].shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.271023Z",
          "iopub.execute_input": "2023-07-11T14:05:46.271641Z",
          "iopub.status.idle": "2023-07-11T14:05:46.282300Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.271588Z",
          "shell.execute_reply": "2023-07-11T14:05:46.280761Z"
        },
        "trusted": true,
        "id": "Ms62_Opxehcy",
        "outputId": "6815f448-e634-415c-aa51-583f4a04ab3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 784) (10000,)\n",
            "(10000, 784) (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vectorizing the labels\n",
        "This convert categorical labels (such as digits 0-9) into a vector numerical format. Later, our model will not directly predict the digits of handwritten images. Rather, our model gives a probability distribution of what digit an image might be. For example, an image of 2 will be [0%,0%, 100%, 0%,0%,0%,0%,0%,0%,0%] meaning it have 100% probability of being a 2, and 0% probability of being other digits.\n",
        "By vectorizing the training data, we make later calculations more efficient."
      ],
      "metadata": {
        "id": "t0iK3TjSehcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorized_result(j):\n",
        "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
        "    position and zeroes elsewhere.  This is used to convert a digit\n",
        "    (0...9) into a corresponding desired output from the neural\n",
        "    network.\"\"\"\n",
        "    unit_vector = np.zeros((10, 1))\n",
        "    unit_vector[j] = 1.0\n",
        "    return unit_vector"
      ],
      "metadata": {
        "_uuid": "b1dabc7a-741d-4c16-8f73-eb2c3e195497",
        "_cell_guid": "b8a9beab-7de5-4a7c-a658-0442aff26fdd",
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.285221Z",
          "iopub.execute_input": "2023-07-11T14:05:46.285771Z",
          "iopub.status.idle": "2023-07-11T14:05:46.294714Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.285720Z",
          "shell.execute_reply": "2023-07-11T14:05:46.293117Z"
        },
        "trusted": true,
        "id": "ggvmYzWqehcz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for example:\n",
        "vectorized_result(2)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.297228Z",
          "iopub.execute_input": "2023-07-11T14:05:46.298811Z",
          "iopub.status.idle": "2023-07-11T14:05:46.314480Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.298751Z",
          "shell.execute_reply": "2023-07-11T14:05:46.312677Z"
        },
        "trusted": true,
        "id": "cafUGfzoehcz",
        "outputId": "2f1dbb34-d96b-407c-a238-ed2ec5e1d77a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.],\n",
              "       [0.],\n",
              "       [1.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "now we apply vectorization to every training label. We also reshape the training inputs from row to column vectors"
      ],
      "metadata": {
        "id": "WUG_YTNiehcz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = []\n",
        "for i in range(50000):\n",
        "    image_vector = training_inputs[i]\n",
        "    # convert the row vectors to column vectors (for the purpose of doing matrix multiplications later)\n",
        "    image_vector = np.reshape(image_vector, (784,1))\n",
        "\n",
        "    image_label = training_results[i]\n",
        "    # vectorize the image labels\n",
        "    image_label = vectorized_result(image_label)\n",
        "\n",
        "    training_set.append((image_vector,image_label))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.316640Z",
          "iopub.execute_input": "2023-07-11T14:05:46.318430Z",
          "iopub.status.idle": "2023-07-11T14:05:46.669111Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.318370Z",
          "shell.execute_reply": "2023-07-11T14:05:46.667386Z"
        },
        "trusted": true,
        "id": "87_adWZgehcz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform the same operation for validation set and testing set.\n",
        "# to save space, the code below is written using Python's list comprehension shortcut.\n",
        "# the outcome is the same as the code above, except the labels are NOT vectorized\n",
        "validation_inputs = [np.reshape(x, (784, 1)) for x in validation_data[0]]\n",
        "validation_set = list(zip(validation_inputs, validation_data[1]))\n",
        "test_inputs = [np.reshape(x, (784, 1)) for x in test_data[0]]\n",
        "test_set = list(zip(test_inputs, test_data[1]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.673558Z",
          "iopub.execute_input": "2023-07-11T14:05:46.673998Z",
          "iopub.status.idle": "2023-07-11T14:05:46.839961Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.673967Z",
          "shell.execute_reply": "2023-07-11T14:05:46.838932Z"
        },
        "trusted": true,
        "id": "6AFpIDbYehc0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network\n",
        "Below implements the stochastic gradient descent learning\n",
        "algorithm for a feedforward neural network.  Gradients are calculated\n",
        "using backpropagation. For the sake of simplicity and readability, the code is not optimized, and omits many desirable performance features.\n",
        "\n",
        "The rough idea is:\n",
        "1. Randomly initialize the weights and biases.\n",
        "2. Compute the gradient of the cost function in respect to the weights and biases for EVERY image. (i.e. computing how we should change the weights and biases so that the network is less wrong in EVERY image)\n",
        "3. Now we know how we should change the weights and biases so that the network is less wrong, we use this gradient to update the weights and biases (minus the weights and biases by the gradients times a tiny number called learning rate)\n",
        "4. Repeat for as many times as time and computation resource permits\n",
        "\n",
        "The above is called gradient descent.\n",
        "However, computing the gradient for EVERY image is often too slow. Therefore, we only use SOME subset of the dataset, which the exact amount is called the mini_batch_size. This is called stochastic gradient descent.\n",
        "\n",
        "![image.png](https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork2.png)\n",
        "\n",
        "Gradient descent is like a smooth ball rolling down the hill perfectly towards the steepest direction. Stochastic gradient descent is like a dice stumbling down the hill, sometimes rolling side ways, sometimes rolling up, but in general still going down."
      ],
      "metadata": {
        "_uuid": "f118c5f1-bd21-46c5-9e51-ea9291797657",
        "_cell_guid": "980cc559-d4ec-4c64-ae5b-e4c5a326e6ff",
        "trusted": true,
        "id": "r23IPKOhehc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### sigmoid function\n",
        "We will use the sigmoid function as our activation function. It is popular in early NN models for several reason.\n",
        "1. It is continuous and differentiable, enabling calculation of gradients.\n",
        "2. It is non-linear, which means it can solve non-linearly separable problem\n",
        "3. It's output is between 0 and 1, which stabilize the training process by preventing large, unbounded values from propagating through the network. It also allows for a natural interpretation of the output as a probability.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork3.png\" alt=\"Image Description\" width=\"50%\">\n",
        "\n",
        "Note: in modern deep neural networks, sigmoid function has been replaced by other functions such as RELU (rectified linear unit).\n"
      ],
      "metadata": {
        "id": "0yhdBC6hehc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.841193Z",
          "iopub.execute_input": "2023-07-11T14:05:46.841750Z",
          "iopub.status.idle": "2023-07-11T14:05:46.848223Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.841718Z",
          "shell.execute_reply": "2023-07-11T14:05:46.846951Z"
        },
        "trusted": true,
        "id": "MX5xxIIeehc1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the network (weights & biases)\n",
        "We will make a network with 3 layers. The input layer have 784 neurons, the middle layer have 15 neurons, and output layer have 10 neurons.\n",
        "\n",
        "We chose 3 layers in this demo just to keep the training time quick. Usually, larger networks of more layers will perform better.\n",
        "\n",
        "![image.png](https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork4.png)"
      ],
      "metadata": {
        "id": "ZQHsyNt8ehc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The biases and weights are all initialized randomly, using the Numpy np.random.randn function to generate Gaussian distributions with mean 0 and standard deviation 1. This random initialization gives our stochastic gradient descent algorithm a place to start from. Biases and weights are stored as lists of Numpy matrices.\n",
        "\n",
        "Since the first layer of neurons is an input layer, we do not set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.\n"
      ],
      "metadata": {
        "id": "DbAZWrRxehc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer1_size, layer2_size, layer3_size = (784, 15, 10)\n",
        "num_layers = 3\n",
        "\n",
        "biases = []\n",
        "\n",
        "# layer 1 bias: first layer of neurons is an input layer, se we don't set any biases for those neurons,\n",
        "# since biases are only ever used in computing the outputs from later layers.\n",
        "layer2_bias = np.random.randn(15,1)\n",
        "biases.append(layer2_bias)\n",
        "layer3_bias = np.random.randn(10,1)\n",
        "biases.append(layer3_bias)\n",
        "\n",
        "weights = []\n",
        "layer1_to_2_weight = np.random.randn(15,784)\n",
        "weights.append(layer1_to_2_weight)\n",
        "layer2_to_3_weight = np.random.randn(10, 15)\n",
        "weights.append(layer2_to_3_weight)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.850891Z",
          "iopub.execute_input": "2023-07-11T14:05:46.851497Z",
          "iopub.status.idle": "2023-07-11T14:05:46.868401Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.851448Z",
          "shell.execute_reply": "2023-07-11T14:05:46.867207Z"
        },
        "trusted": true,
        "id": "g44MEuLmehc2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feedforward\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork5.png\" alt=\"Image Description\" width=\"20%\">\n",
        "\n",
        "a is the vector of activations of the n-th layer of neurons. To obtain a′ (n+1 th layer),\n",
        "we multiply a by the weight matrix w, and add the vector b of biases. We then apply the function σ elementwise to every entry in the vector wa+b.\n",
        "\n",
        "Because we have 3 layers of neurons, we need to apply this process twice (layer1-->2, layer2-->3)"
      ],
      "metadata": {
        "id": "7UK7iVtoehc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feedforward(a, biases, weights):\n",
        "    #layer 1 --> 2\n",
        "    b = biases[0]\n",
        "    w = weights[0]\n",
        "    a_layer2 = sigmoid(np.dot(w, a) + b)\n",
        "\n",
        "    #layer 2 --> 3\n",
        "    b = biases[1]\n",
        "    w = weights[1]\n",
        "    a_layer3 = sigmoid(np.dot(w, a_layer2) + b)\n",
        "    return a_layer3"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.869932Z",
          "iopub.execute_input": "2023-07-11T14:05:46.871288Z",
          "iopub.status.idle": "2023-07-11T14:05:46.879952Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.871248Z",
          "shell.execute_reply": "2023-07-11T14:05:46.878666Z"
        },
        "trusted": true,
        "id": "6OHfLHpDehc3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating accuracy (to count the amount of correct predictions)\n"
      ],
      "metadata": {
        "id": "fRAD_2Cjehc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(test_data, biases, weights):\n",
        "    correct_prediction_count = 0\n",
        "    for x, y in test_data:\n",
        "        # example: [0, 0, 0.9, 0.05, 0.05, 0, 0,0 ,0,0]\n",
        "        predicted_y = np.argmax(feedforward(x, biases, weights))\n",
        "        if predicted_y == y:\n",
        "            correct_prediction_count +=1\n",
        "    return correct_prediction_count"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.881770Z",
          "iopub.execute_input": "2023-07-11T14:05:46.882206Z",
          "iopub.status.idle": "2023-07-11T14:05:46.903427Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.882168Z",
          "shell.execute_reply": "2023-07-11T14:05:46.901426Z"
        },
        "trusted": true,
        "id": "HiV3I_GXehc3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy)"
      ],
      "metadata": {
        "id": "f6675qdB4sMc",
        "outputId": "e07ddca1-da83-4e05-924c-5aae03d551fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic gradient descent\n",
        "To speed up training, we only use a subset of all the data during one iteration. These subsets are called mini-batches.\n",
        "In each epoch, we start by randomly shuffling the training data. Then, we slice it into mini-batches. Then for each mini_batch we apply a single step of gradient descent, which updates the network weights and biases according to a single iteration of gradient descent, using just the training data in mini_batch."
      ],
      "metadata": {
        "id": "eW7Ewu4iehc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SGD(training_data, num_epochs, mini_batch_size, learning_rate, num_layers, test_data=None):\n",
        "    #the purpose of SGD is to change the biases and weights\n",
        "    global biases, weights\n",
        "\n",
        "    n = len(training_data)\n",
        "\n",
        "    for j in range(num_epochs):\n",
        "        time1 = time.time()\n",
        "\n",
        "        random.shuffle(training_data)\n",
        "\n",
        "        for k in range(0, n, mini_batch_size): # for(int k=0; k<50000; k+=mini_batch_size)\n",
        "            #slice the dataset into batches\n",
        "            mini_batch = training_data[k:k + mini_batch_size]\n",
        "            #apply a single step of gradient descent\n",
        "            biases, weights = update_mini_batch(mini_batch, learning_rate, biases, weights, num_layers)\n",
        "\n",
        "        time2 = time.time()\n",
        "\n",
        "        count = evaluate(test_data, biases, weights)\n",
        "        accuracy = count / len(test_data)\n",
        "        print(f\"Epoch {j}: {count} / {len(test_data)}, took {(time2-time1):.2f} seconds\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.905046Z",
          "iopub.execute_input": "2023-07-11T14:05:46.906053Z",
          "iopub.status.idle": "2023-07-11T14:05:46.917875Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.906011Z",
          "shell.execute_reply": "2023-07-11T14:05:46.916555Z"
        },
        "trusted": true,
        "id": "gCH7iwrwehc4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a minibatch for SGD\n",
        "This function computes the gradients for every training image in the mini_batch, and then updating weights and biases appropriately.\n"
      ],
      "metadata": {
        "id": "DaJrdtNDehc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_mini_batch(mini_batch, learning_rate, biases, weights, num_layers):\n",
        "    mini_batch_size = len(mini_batch)\n",
        "\n",
        "    # initializes nabla_b and nabla_w, with arrays of zeros, and same shape as the biases and weights.\n",
        "    \"\"\"\n",
        "    simplified version:\n",
        "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
        "    explicit version:\n",
        "    \"\"\"\n",
        "    nabla_b = []\n",
        "    nabla_w = []\n",
        "    for b in biases:\n",
        "        nabla_b.append(np.zeros(b.shape))\n",
        "    for w in weights:\n",
        "        nabla_w.append(np.zeros(w.shape))\n",
        "\n",
        "    #computes the gradients (of cost function) for every training image in the mini_batch.\n",
        "    for x, y in mini_batch:\n",
        "        delta_nabla_b, delta_nabla_w = backprop(x, y, num_layers, biases, weights)\n",
        "\n",
        "        for i in range(len(nabla_b)):\n",
        "            nabla_b[i] += delta_nabla_b[i]\n",
        "\n",
        "        for i in range(len(nabla_w)):\n",
        "            nabla_w[i] += delta_nabla_w[i]\n",
        "\n",
        "    # updating weights and biases appropriately:\n",
        "    # (take a step in the direction opposite to the gradient, with step size proportional to the learning_rate)\n",
        "    # we divide by mini_batch_size to obtain average\n",
        "    for i in range(len(weights)):\n",
        "        weights[i] -= (learning_rate / mini_batch_size) * nabla_w[i]\n",
        "    for i in range(len(biases)):\n",
        "        biases[i] -= (learning_rate / mini_batch_size) * nabla_b[i]\n",
        "\n",
        "    return biases, weights\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.919622Z",
          "iopub.execute_input": "2023-07-11T14:05:46.921290Z",
          "iopub.status.idle": "2023-07-11T14:05:46.936735Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.921235Z",
          "shell.execute_reply": "2023-07-11T14:05:46.935383Z"
        },
        "trusted": true,
        "id": "SgW0Y-Hlehc4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation\n",
        "The backpropagation algorithm is a fast way of computing the gradient of the cost function. It takes in a single example and returns the gradient of the cost function in respect to the weights and biases. In other words, we give this function an image + label, and the function tells us how the biases and weights should be altered such that the biases and weights can correctly classify this image.\n",
        "\n",
        "The activation of every layer depends on the previous layer:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork6.png\" alt=\"Image Description\" width=\"17%\">\n",
        "\n",
        "We will name the inside part \"weighted input\", or z\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork7.png\" alt=\"Image Description\" width=\"15%\">\n",
        "\n",
        "The following are the fundamental equations of back propagation. Proofs can be found here: http://neuralnetworksanddeeplearning.com/chap2.html\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork8.png\" alt=\"Image Description\" width=\"40%\">\n"
      ],
      "metadata": {
        "id": "JFR_HleCehc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_derivative(output_activations, y):\n",
        "    return output_activations - y\n",
        "\n",
        "def backprop(x, y, num_layers, biases, weights):\n",
        "    import numpy as np\n",
        "\n",
        "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
        "\n",
        "    activation = x\n",
        "    activation_layers = [x]  # to store activations layer by layer\n",
        "    z_layers = []            # to store z vectors layer by layer\n",
        "\n",
        "    # Forward pass (across all layers)\n",
        "    for b, w in zip(biases, weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        z_layers.append(z)\n",
        "        activation = sigmoid(z)\n",
        "        activation_layers.append(activation)\n",
        "\n",
        "    # Backward pass (last layer first)\n",
        "    delta = cost_derivative(activation_layers[-1], y) * sigmoid_prime(z_layers[-1])\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activation_layers[-2].transpose())\n",
        "\n",
        "    # Loop backwards through previous layers\n",
        "    for l in range(2, num_layers):\n",
        "        z = z_layers[-l]\n",
        "        sp = sigmoid_prime(z)\n",
        "        delta = np.dot(weights[-l + 1].T, delta) * sp\n",
        "        nabla_b[-l] = delta\n",
        "        nabla_w[-l] = np.dot(delta, activation_layers[-l - 1].T)\n",
        "\n",
        "    return nabla_b, nabla_w\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:05:46.938564Z",
          "iopub.execute_input": "2023-07-11T14:05:46.939296Z",
          "iopub.status.idle": "2023-07-11T14:05:46.957378Z",
          "shell.execute_reply.started": "2023-07-11T14:05:46.939258Z",
          "shell.execute_reply": "2023-07-11T14:05:46.956122Z"
        },
        "trusted": true,
        "id": "T1i9i5rqehc5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "mini_batch_size = 5\n",
        "learning_rate = 0.2\n",
        "SGD(training_set, epochs, mini_batch_size, learning_rate, num_layers,test_set)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:54:25.600518Z",
          "iopub.execute_input": "2023-07-11T14:54:25.601011Z",
          "iopub.status.idle": "2023-07-11T14:57:14.082479Z",
          "shell.execute_reply.started": "2023-07-11T14:54:25.600977Z",
          "shell.execute_reply": "2023-07-11T14:57:14.080878Z"
        },
        "trusted": true,
        "id": "3dUnCcvzehc5",
        "outputId": "2e6b58d8-68e9-4564-ddc2-573e548cfaae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-13-939139594.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 1911 / 10000, took 3.29 seconds\n",
            "Epoch 1: 1911 / 10000, took 3.15 seconds\n",
            "Epoch 2: 1911 / 10000, took 4.47 seconds\n",
            "Epoch 3: 1911 / 10000, took 3.14 seconds\n",
            "Epoch 4: 1911 / 10000, took 3.22 seconds\n",
            "Epoch 5: 1911 / 10000, took 3.70 seconds\n",
            "Epoch 6: 1911 / 10000, took 4.26 seconds\n",
            "Epoch 7: 1911 / 10000, took 3.11 seconds\n",
            "Epoch 8: 1911 / 10000, took 3.24 seconds\n",
            "Epoch 9: 1911 / 10000, took 4.62 seconds\n",
            "Epoch 10: 1911 / 10000, took 3.15 seconds\n",
            "Epoch 11: 1911 / 10000, took 3.34 seconds\n",
            "Epoch 12: 1911 / 10000, took 4.11 seconds\n",
            "Epoch 13: 1911 / 10000, took 3.27 seconds\n",
            "Epoch 14: 1911 / 10000, took 3.21 seconds\n",
            "Epoch 15: 1911 / 10000, took 3.19 seconds\n",
            "Epoch 16: 1911 / 10000, took 4.36 seconds\n",
            "Epoch 17: 1911 / 10000, took 3.13 seconds\n",
            "Epoch 18: 1911 / 10000, took 3.19 seconds\n",
            "Epoch 19: 1911 / 10000, took 4.07 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: training this model takes a lot more time than other models we have seen in the past. In a later session, we will learn to use GPU's to make this faster."
      ],
      "metadata": {
        "id": "kmHQPLJYjNIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = evaluate(validation_set, biases, weights) / len(validation_set)\n",
        "print(\"accuracy\",accuracy)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:07:18.935717Z",
          "iopub.execute_input": "2023-07-11T14:07:18.937246Z",
          "iopub.status.idle": "2023-07-11T14:07:19.854214Z",
          "shell.execute_reply.started": "2023-07-11T14:07:18.937169Z",
          "shell.execute_reply": "2023-07-11T14:07:19.852271Z"
        },
        "trusted": true,
        "id": "ZTEcmOgYehc5",
        "outputId": "0a4f9557-cc16-4780-e2ba-9ff6a5cb80d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-13-939139594.py:2: RuntimeWarning: overflow encountered in exp\n",
            "  return 1.0 / (1.0 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.1993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercises\n",
        "1. Change the middle layer of the network from 15 neurons to 30 neurons, and observe the result\n",
        "2. Explain how does learning_rate affect the training process\n",
        "3. Explain how does epoch count affect the training process\n",
        "4. Bonus: change the network to 4 layers, with two middle layers both with 30 neurons.\n",
        "5. Bonus: choose another activation function here, and replace the sigmoid & sigmoid_price functions with it.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/MLcmore2023/MLcmore2023/main/.images/neuralnetwork9.png\" alt=\"Image Description\" width=\"50%\">\n"
      ],
      "metadata": {
        "id": "eC73YrQKehc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Change the middle layer of the network from 15 neurons to 30 neurons, and observe the result**\n",
        "\n",
        "* **설명**:\n",
        "  중간 은닉층의 뉴런 수를 15개에서 30개로 늘리면 모델의 표현력이 증가합니다.\n",
        "  복잡한 함수나 패턴을 더 잘 학습할 수 있지만, 파라미터 수가 증가하므로:\n",
        "\n",
        "  * **장점**: 더 복잡한 데이터 패턴 학습 가능\n",
        "  * **단점**: 과적합(overfitting) 가능성 증가, 학습 속도 저하 가능\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Explain how does `learning_rate` affect the training process**\n",
        "\n",
        "* **설명**:\n",
        "  학습률(learning rate)은 경사하강법에서 얼마나 크게 가중치를 조정할지를 결정합니다.\n",
        "\n",
        "  * **너무 크면**: 발산하거나 최솟값을 지나칠 수 있음\n",
        "  * **너무 작으면**: 수렴 속도가 매우 느려지고, 지역 최솟값에 갇힐 수 있음\n",
        "    적절한 학습률은 안정적이고 빠른 수렴을 가능하게 합니다.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Explain how does `epoch` count affect the training process**\n",
        "\n",
        "* **설명**:\n",
        "  Epoch는 전체 학습 데이터를 한 번 네트워크에 학습시키는 횟수입니다.\n",
        "\n",
        "  * **적으면**: 학습 부족(underfitting) 가능\n",
        "  * **너무 많으면**: 과적합(overfitting) 가능\n",
        "    적절한 epoch 수는 검증 데이터의 성능으로 판단해야 하며, 일반적으로 **조기 종료(Early stopping)** 등의 전략도 함께 사용합니다.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Bonus: change the network to 4 layers, with two middle layers both with 30 neurons**\n",
        "\n",
        "* **설명**:\n",
        "  네트워크를 4층 구조로 만들고, 중간 두 은닉층을 각각 30 뉴런으로 구성하면 깊은 신경망이 됩니다.\n",
        "\n",
        "  * **장점**: 더 높은 표현력, 더 복잡한 비선형 관계 학습 가능\n",
        "  * **단점**: 더 많은 학습 데이터와 정규화 전략 필요, 계산량 증가\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Bonus: choose another activation function here, and replace the `sigmoid` & `sigmoid_prime` functions with it**\n",
        "\n",
        "* **설명**:\n",
        "  예시 이미지에 나온 다른 활성화 함수 (예: ReLU, tanh, Leaky ReLU 등) 중 하나를 선택해 `sigmoid`, `sigmoid_prime` 함수를 해당 함수로 교체합니다.\n",
        "\n",
        "  예: ReLU로 교체 시\n",
        "\n",
        "  ```python\n",
        "  def relu(z):\n",
        "      return np.maximum(0, z)\n",
        "\n",
        "  def relu_prime(z):\n",
        "      return (z > 0).astype(float)\n",
        "  ```\n",
        "\n",
        "  * **ReLU 장점**: 계산 간단, gradient vanishing 문제 완화\n",
        "  * **단점**: 일부 뉴런이 완전히 죽는 문제(Dead ReLU)\n",
        "\n"
      ],
      "metadata": {
        "id": "IqG5U3QP-BCw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Change the middle layer of the network from 15 neurons to 30 neurons, and observe the result**\n",
        "\n",
        "* **Explanation**:\n",
        "  Increasing the number of neurons in the hidden layer from 15 to 30 improves the model's expressive power.\n",
        "  It enables the model to learn more complex functions or patterns, but increases the number of parameters.\n",
        "\n",
        "  * **Advantage**: Can learn more complex data patterns\n",
        "  * **Disadvantage**: Increased risk of overfitting, slower training\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Explain how does `learning_rate` affect the training process**\n",
        "\n",
        "* **Explanation**:\n",
        "  The learning rate determines how much the weights are updated during each step of gradient descent.\n",
        "\n",
        "  * **Too large**: May diverge or overshoot the minimum\n",
        "  * **Too small**: Convergence becomes very slow and may get stuck in a local minimum\n",
        "    A proper learning rate allows for stable and efficient convergence.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Explain how does `epoch` count affect the training process**\n",
        "\n",
        "* **Explanation**:\n",
        "  An epoch is one complete pass through the entire training dataset.\n",
        "\n",
        "  * **Too few**: May result in underfitting\n",
        "  * **Too many**: May result in overfitting\n",
        "    The optimal number of epochs is usually determined based on validation performance, often with techniques like **early stopping**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Bonus: change the network to 4 layers, with two middle layers both with 30 neurons**\n",
        "\n",
        "* **Explanation**:\n",
        "  Changing the network to a 4-layer structure with two hidden layers of 30 neurons each creates a deeper neural network.\n",
        "\n",
        "  * **Advantage**: Higher expressive power, better at learning complex non-linear relationships\n",
        "  * **Disadvantage**: Requires more training data and regularization, increases computational cost\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Bonus: choose another activation function here, and replace the `sigmoid` & `sigmoid_prime` functions with it**\n",
        "\n",
        "* **Explanation**:\n",
        "  Choose another activation function from the image (e.g., ReLU, tanh, Leaky ReLU) and replace the `sigmoid` and `sigmoid_prime` functions accordingly.\n",
        "\n",
        "  Example: If replacing with ReLU\n",
        "\n",
        "  ```python\n",
        "  def relu(z):\n",
        "      return np.maximum(0, z)\n",
        "\n",
        "  def relu_prime(z):\n",
        "      return (z > 0).astype(float)\n",
        "  ```\n",
        "\n",
        "  * **ReLU Advantage**: Simple to compute, helps avoid gradient vanishing\n",
        "  * **Disadvantage**: Some neurons may die completely (Dead ReLU problem)\n",
        "\n"
      ],
      "metadata": {
        "id": "4VKrTOGU_uE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    return (z > 0).astype(float)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T14:07:19.857400Z",
          "iopub.execute_input": "2023-07-11T14:07:19.859176Z",
          "iopub.status.idle": "2023-07-11T14:07:19.873590Z",
          "shell.execute_reply.started": "2023-07-11T14:07:19.858981Z",
          "shell.execute_reply": "2023-07-11T14:07:19.872106Z"
        },
        "trusted": true,
        "id": "t7PqnPNoehc6"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def cost_derivative(output_activations, y):\n",
        "    return output_activations - y\n",
        "\n",
        "def backprop(x, y, num_layers, biases, weights):\n",
        "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
        "\n",
        "    activation = x\n",
        "    activation_layers = [x]\n",
        "    z_layers = []\n",
        "\n",
        "    # Forward pass using ReLU\n",
        "    for b, w in zip(biases, weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        z_layers.append(z)\n",
        "        activation = relu(z)   # CHANGED: from sigmoid to relu\n",
        "        activation_layers.append(activation)\n",
        "\n",
        "    # Backward pass using ReLU derivative\n",
        "    delta = cost_derivative(activation_layers[-1], y) * relu_prime(z_layers[-1])  # CHANGED\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activation_layers[-2].transpose())\n",
        "\n",
        "    for l in range(2, num_layers):\n",
        "        z = z_layers[-l]\n",
        "        sp = relu_prime(z)  # CHANGED\n",
        "        delta = np.dot(weights[-l + 1].T, delta) * sp\n",
        "        nabla_b[-l] = delta\n",
        "        nabla_w[-l] = np.dot(delta, activation_layers[-l - 1].T)\n",
        "\n",
        "    return nabla_b, nabla_w\n"
      ],
      "metadata": {
        "id": "WCcsTAr8F9cq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def relu_prime(z):\n",
        "    return (z > 0).astype(float)\n",
        "\n",
        "def cost_derivative(output_activations, y):\n",
        "    return output_activations - y\n",
        "\n",
        "def backprop(x, y, num_layers, biases, weights):\n",
        "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
        "\n",
        "    activation = x\n",
        "    activation_layers = [x]\n",
        "    z_layers = []\n",
        "\n",
        "    # Forward pass using ReLU\n",
        "    for b, w in zip(biases, weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        z_layers.append(z)\n",
        "        activation = relu(z)\n",
        "        activation_layers.append(activation)\n",
        "\n",
        "    # Backward pass\n",
        "    delta = cost_derivative(activation_layers[-1], y) * relu_prime(z_layers[-1])\n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activation_layers[-2].T)\n",
        "\n",
        "    for l in range(2, num_layers):\n",
        "        z = z_layers[-l]\n",
        "        sp = relu_prime(z)\n",
        "        delta = np.dot(weights[-l + 1].T, delta) * sp\n",
        "        nabla_b[-l] = delta\n",
        "        nabla_w[-l] = np.dot(delta, activation_layers[-l - 1].T)\n",
        "\n",
        "    return nabla_b, nabla_w\n",
        "\n",
        "# 정확도 계산 함수 추가\n",
        "def compute_accuracy(data, biases, weights):\n",
        "    correct = 0\n",
        "    for x, y in data:\n",
        "        a = x\n",
        "        for b, w in zip(biases, weights):\n",
        "            a = relu(np.dot(w, a) + b)\n",
        "        predicted = np.argmax(a)\n",
        "        actual = np.argmax(y)\n",
        "        if predicted == actual:\n",
        "            correct += 1\n",
        "    return correct / len(data)\n"
      ],
      "metadata": {
        "id": "v_oj-x_-G24U"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### References\n",
        "- http://neuralnetworksanddeeplearning.com/\n",
        "- https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/\n",
        "- https://www.3blue1brown.com/topics/neural-networks\n",
        "- https://towardsdatascience.com/stochastic-gradient-descent-clearly-explained-53d239905d31?gi=7c68464316bd"
      ],
      "metadata": {
        "id": "5X2qhmFpehdC"
      }
    }
  ]
}