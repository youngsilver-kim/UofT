{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ic1aqheRiS0"
   },
   "source": [
    "#### 00. ë­ì²´ì¸ ë° ì™¸ë¶€ ëª¨ë¸(Gemini) ì œê³µ íŒ¨í‚¤ì§€ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wDFjgrel8C1"
   },
   "outputs": [],
   "source": [
    "# ë­ì²´ì¸ ì„¤ì¹˜\n",
    "!pip install langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LNLQ-UaXoH5"
   },
   "outputs": [],
   "source": [
    "# êµ¬ê¸€ Geminië¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5AVrJq3Rcm7"
   },
   "outputs": [],
   "source": [
    "# Google Gemini í…ŒìŠ¤íŠ¸\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=\"AIzaSyDwmO4ICWgVcmG2r4a73j9Wy1GkWEwTV_U\")\n",
    "llm_msg = llm.invoke(\"Gemini ìš”ê¸ˆì œì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜\")\n",
    "print(llm_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY3J02PBSiBb"
   },
   "source": [
    "#### 01. ì²´ì¸ ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0PVWKmvRhcE"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# 1. llm, prompt, parser ì •ì˜\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=\"AIzaSyDwmO4ICWgVcmG2r4a73j9Wy1GkWEwTV_U\")\n",
    "prompt = ChatPromptTemplate.from_template(\"ë„ˆëŠ” ì „ì£¼ëŒ€í•™êµ í•™ìƒì´ì•¼. ë‹¤ìŒì˜ ì§ˆë¬¸ì„ ì‰½ê²Œ ë‹µë³€í•´ì¤˜ : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 2. ì²´ì¸ ìƒì„±\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 3. ì²´ì¸ ì‹¤í–‰\n",
    "result = chain.invoke({\"input\": \"ì „ì£¼ëŒ€í•™êµì— ëŒ€í•´ ì†Œê°œí•´ì¤˜\"})\n",
    "print(\"ì‘ë‹µ ê²°ê³¼:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9yyyGkoiRem"
   },
   "source": [
    "#### 02. ë¬¸ì„œ ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytn1UVeOlM-N"
   },
   "outputs": [],
   "source": [
    "# pdf ë¡œë” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install langchain pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FW8qB8FVIeN"
   },
   "outputs": [],
   "source": [
    "# PDF íŒŒì¼ ì—…ë¡œë“œ\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()  # íŒŒì¼ ì„ íƒ ì°½ì´ ì—´ë¦¬ë©°, PDF íŒŒì¼ë“¤ì„ ì„ íƒí•˜ì—¬ ì—…ë¡œë“œí•˜ê³  ë“œë¼ì´ë¸Œì—ì„œ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CmWtLGFm7Lt"
   },
   "outputs": [],
   "source": [
    "# PyPDFLoaderë¥¼ ì´ìš©í•œ ì—¬ëŸ¬ PDF íŒŒì¼ë“¤ì„ ë¡œë”©\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì˜ PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°\n",
    "pdf_files = [f for f in os.listdir() if f.lower().endswith('.pdf')]\n",
    "\n",
    "all_documents = []\n",
    "for filename in pdf_files:\n",
    "      loader = PyPDFLoader(filename)\n",
    "      docs = loader.load()\n",
    "      all_documents.extend(docs)\n",
    "\n",
    "print(f\"ì´ ë¬¸ì„œ ìˆ˜: {len(all_documents)}\")\n",
    "print(f\"ì²«ë²ˆì§¸ ë¬¸ì„œ: {all_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXFjasHdtb3D"
   },
   "source": [
    "#### 03. ë¬¸ì„œ ì²­í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZX6oFZWpiTk"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(split_docs)}\")\n",
    "print(\"***** ì²«ë²ˆì§¸ ì²­í¬ *****\\n\")\n",
    "print(split_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrbBVWXyv6lq"
   },
   "source": [
    "#### 04. ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu4pOTaStka9"
   },
   "outputs": [],
   "source": [
    "# ì„ë² ë”©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1T-e_9mw3xL"
   },
   "outputs": [],
   "source": [
    "# ì„ë² ë”©\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# ë¬´ë£Œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© ëª¨ë¸ ì •ì˜\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sroberta-nli',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te1s3AqdyH1y"
   },
   "outputs": [],
   "source": [
    "# Chroma ë²¡í„° DBì— ì €ì¥\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# ë””ìŠ¤í¬ì— ì €ì¥í•  ë””ë ‰í† ë¦¬ ì§€ì •\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOlSGIzVcOk8"
   },
   "source": [
    "#### 05. ê²€ìƒ‰(Retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHKmxA-rcNSV"
   },
   "outputs": [],
   "source": [
    "query = 'ì–´ëŠ í•™ê³¼ì—ì„œ ì¸ê³µì§€ëŠ¥ì„ ì˜ ê°€ë¥´ì¹˜ë‚˜ìš”?'\n",
    "\n",
    "# ê°€ì¥ ìœ ì‚¬ë„ê°€ ë†’ì€ ì²­í¬ë¥¼ 3ê°œ ì¶”ì¶œí•˜ëŠ” Retriever ê°ì²´ ìƒì„±\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "# ìë£Œë¥¼ ì¶”ì¶œ ë° í™•ì¸\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(f'ì²­í¬ ê°œìˆ˜: {len(docs)}')\n",
    "print(f'ì²«ë²ˆì§¸ ì²­í¬ë‚´ìš©: {docs[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBkzqhTa2B2k"
   },
   "source": [
    "#### 06. LLMì—ê²Œ ê´€ë ¨ ì²­í¬ì™€ í•¨ê»˜ ì§ˆì˜í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_On23-E2KGL"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„± (Prompt)\n",
    "template = '''ë‹¤ìŒì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”: {context}\n",
    "ì§ˆë¬¸: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# ëª¨ë¸ (Model)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                             api_key=\"AIzaSyD9u7xQibC5jNJTz_qSEHBAORSUwWSe58w\",\n",
    "                             temperature=0, max_tokens=500,)\n",
    "\n",
    "# ì²´ì¸ êµ¬ì„± (Chain Execution)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# ë¬¸ì„œ í¬ë§·íŒ… (Formatting Docs)\n",
    "def format_docs(docs): # ì²­í¬ ì‚¬ì´ì— '\\n\\n'ì„ ì‚½ì…\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "# ì‹¤í–‰ (LLMì—ê²Œ ì§ˆì˜í•˜ê¸°)\n",
    "response = chain.invoke({'context': (format_docs(docs)), 'question':query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhHiFUyp7aNU"
   },
   "source": [
    "#### 07. ì±—ë´‡ UI ë§Œë“¤ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4CQHey0P1ds"
   },
   "source": [
    "##### í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SINqrINo7c4s"
   },
   "outputs": [],
   "source": [
    "# streamlit ì„¤ì¹˜\n",
    "!pip install streamlit streamlit-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx0C-W8iDA56"
   },
   "outputs": [],
   "source": [
    "# Colab ëŸ°íƒ€ì„ì—ì„œ ì‹¤í–‰ë˜ëŠ” Streamlit ì•±ì„ ì™¸ë¶€ì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í„°ë„ì„ ìƒì„±\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmKphN3--Y3q"
   },
   "outputs": [],
   "source": [
    "# ê°„ë‹¨í•œ í”„ë¡œê·¸ë¨(app.py) ì‘ì„±\n",
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "message(\"ì±—ë´‡ ë©”ì‹œì§€\")\n",
    "message(\"ì•ˆë…•í•˜ì„¸ìš”, ë´‡!\", is_user=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3uAF8Y1-y-J"
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "PORT = 8501\n",
    "\n",
    "# Streamlit ì•±ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰(í¬íŠ¸: 8501)\n",
    "process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", str(PORT), \"--server.enableCORS\", \"true\", \"--server.enableXsrfProtection\", \"false\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# ngrok ì¸ì¦ í† í° ì„¤ì • (ngrok í™ˆì—ì„œ ê³„ì •ìƒì„± í›„, ë°›ì„ ìˆ˜ ìˆìŒ)\n",
    "ngrok.set_auth_token(\"2yM2FbBUyooXRCAtvsJfSnsM2Uw_4zXFe5a4ChPKYpSqcvK4X\")\n",
    "\n",
    "# ngrok í„°ë„ ìƒì„±\n",
    "public_url = ngrok.connect(PORT, bind_tls=True)\n",
    "print(f\"Streamlit ì•±ì— ì ‘ì†í•˜ë ¤ë©´ ë‹¤ìŒ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2p_lnlXM4tW"
   },
   "outputs": [],
   "source": [
    "# ngrok í„°ë„ë§ ì¢…ë£Œë¥¼ ìœ„í•œ í•¨ìˆ˜ (ì„ íƒ ì‚¬í•­)\n",
    "# Colab ì„¸ì…˜ì´ ëŠê¸°ë©´ ìë™ìœ¼ë¡œ ì¢…ë£Œë˜ì§€ë§Œ, ìˆ˜ë™ìœ¼ë¡œ ì¢…ë£Œí•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©\n",
    "def kill_streamlit_ngrok():\n",
    "    process.terminate()\n",
    "    print(\"Streamlit í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ...\")\n",
    "    ngrok.kill()\n",
    "    print(\"ngrok í„°ë„ ì¢…ë£Œ...\")\n",
    "\n",
    "# kill_streamlit_ngrok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjslsMExUpRh"
   },
   "source": [
    "#### ì˜ˆì œ: Geminiì™€ ì—°ë™í•˜ëŠ” ì±—ë´‡ í”„ë¡œê·¸ë¨(chat.py) ì‘ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcdIK7iMODHs"
   },
   "outputs": [],
   "source": [
    "%%writefile chat.py\n",
    "import streamlit as st\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# íƒ€ì´í‹€ ì§€ì •\n",
    "st.title(\"ğŸ’¬ Streamlit ì±—ë´‡\")\n",
    "\n",
    "# ì´ˆê¸°í™”\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"assistant\", \"content\": \"ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\"}\n",
    "    ]\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                             api_key=\"AIzaSyD9u7xQibC5jNJTz_qSEHBAORSUwWSe58w\",)\n",
    "\n",
    "def ask_llm(prompt):\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì±—ë´‡ì…ë‹ˆë‹¤.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}, ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.write(msg[\"content\"])\n",
    "\n",
    "if query := st.chat_input(\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "    st.chat_message(\"user\").write(query)\n",
    "    response = ask_llm(query)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    st.chat_message(\"assistant\").write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dileiIeXV7xT"
   },
   "outputs": [],
   "source": [
    "# ì‹¤í–‰ í”„ë¡œê·¸ë¨ ì‘ì„±\n",
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "PORT= \"8501\"\n",
    "\n",
    "# Streamlit ì•±ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰(í¬íŠ¸: 8501)\n",
    "process = subprocess.Popen([\"streamlit\", \"run\", \"chat.py\", \"--server.port\", PORT, \"--server.enableCORS\", \"true\", \"--server.enableXsrfProtection\", \"false\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# ngrok ì¸ì¦ í† í° ì„¤ì •(ngrok í™ˆì—ì„œ ê³„ì •ìƒì„± í›„, ë°›ì„ ìˆ˜ ìˆìŒ)\n",
    "ngrok.set_auth_token(\"2yM2FbBUyooXRCAtvsJfSnsM2Uw_4zXFe5a4ChPKYpSqcvK4X\")\n",
    "\n",
    "# ngrok í„°ë„ ìƒì„±\n",
    "public_url = ngrok.connect(PORT, bind_tls=True)\n",
    "print(f\"Streamlit ì•± ì ‘ì† ë§í¬: {public_url}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
