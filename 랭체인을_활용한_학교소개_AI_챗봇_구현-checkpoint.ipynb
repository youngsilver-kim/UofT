{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ic1aqheRiS0"
   },
   "source": [
    "#### 00. 랭체인 및 외부 모델(Gemini) 제공 패키지 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wDFjgrel8C1"
   },
   "outputs": [],
   "source": [
    "# 랭체인 설치\n",
    "!pip install langchain langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LNLQ-UaXoH5"
   },
   "outputs": [],
   "source": [
    "# 구글 Gemini를 불러올 수 있는 라이브러리 설치\n",
    "!pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5AVrJq3Rcm7"
   },
   "outputs": [],
   "source": [
    "# Google Gemini 테스트\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=\"AIzaSyDwmO4ICWgVcmG2r4a73j9Wy1GkWEwTV_U\")\n",
    "llm_msg = llm.invoke(\"Gemini 요금제에 대해 설명해줘\")\n",
    "print(llm_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CY3J02PBSiBb"
   },
   "source": [
    "#### 01. 체인 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0PVWKmvRhcE"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# 1. llm, prompt, parser 정의\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", api_key=\"AIzaSyDwmO4ICWgVcmG2r4a73j9Wy1GkWEwTV_U\")\n",
    "prompt = ChatPromptTemplate.from_template(\"너는 전주대학교 학생이야. 다음의 질문을 쉽게 답변해줘 : {input}\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 2. 체인 생성\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 3. 체인 실행\n",
    "result = chain.invoke({\"input\": \"전주대학교에 대해 소개해줘\"})\n",
    "print(\"응답 결과:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9yyyGkoiRem"
   },
   "source": [
    "#### 02. 문서 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytn1UVeOlM-N"
   },
   "outputs": [],
   "source": [
    "# pdf 로더 라이브러리 설치\n",
    "!pip install langchain pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FW8qB8FVIeN"
   },
   "outputs": [],
   "source": [
    "# PDF 파일 업로드\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()  # 파일 선택 창이 열리며, PDF 파일들을 선택하여 업로드하고 드라이브에서 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CmWtLGFm7Lt"
   },
   "outputs": [],
   "source": [
    "# PyPDFLoader를 이용한 여러 PDF 파일들을 로딩\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# 현재 작업 디렉토리의 PDF 파일 목록 가져오기\n",
    "pdf_files = [f for f in os.listdir() if f.lower().endswith('.pdf')]\n",
    "\n",
    "all_documents = []\n",
    "for filename in pdf_files:\n",
    "      loader = PyPDFLoader(filename)\n",
    "      docs = loader.load()\n",
    "      all_documents.extend(docs)\n",
    "\n",
    "print(f\"총 문서 수: {len(all_documents)}\")\n",
    "print(f\"첫번째 문서: {all_documents[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXFjasHdtb3D"
   },
   "source": [
    "#### 03. 문서 청킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZX6oFZWpiTk"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "split_docs = splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"분할된 문서 수: {len(split_docs)}\")\n",
    "print(\"***** 첫번째 청크 *****\\n\")\n",
    "print(split_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrbBVWXyv6lq"
   },
   "source": [
    "#### 04. 임베딩 및 벡터 스토어 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wu4pOTaStka9"
   },
   "outputs": [],
   "source": [
    "# 임베딩을 위한 라이브러리 설치\n",
    "!pip install sentence-transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1T-e_9mw3xL"
   },
   "outputs": [],
   "source": [
    "# 임베딩\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 무료 사용 가능한 임베딩 모델 정의\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name='jhgan/ko-sroberta-nli',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te1s3AqdyH1y"
   },
   "outputs": [],
   "source": [
    "# Chroma 벡터 DB에 저장\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 디스크에 저장할 디렉토리 지정\n",
    "persist_directory = \"./chroma_db\"\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOlSGIzVcOk8"
   },
   "source": [
    "#### 05. 검색(Retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHKmxA-rcNSV"
   },
   "outputs": [],
   "source": [
    "query = '어느 학과에서 인공지능을 잘 가르치나요?'\n",
    "\n",
    "# 가장 유사도가 높은 청크를 3개 추출하는 Retriever 객체 생성\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "# 자료를 추출 및 확인\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "print(f'청크 개수: {len(docs)}')\n",
    "print(f'첫번째 청크내용: {docs[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBkzqhTa2B2k"
   },
   "source": [
    "#### 06. LLM에게 관련 청크와 함께 질의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_On23-E2KGL"
   },
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# 프롬프트 생성 (Prompt)\n",
    "template = '''다음의 내용을 기반으로 하여 질문에 답하세요: {context}\n",
    "질문: {question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 모델 (Model)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                             api_key=\"AIzaSyD9u7xQibC5jNJTz_qSEHBAORSUwWSe58w\",\n",
    "                             temperature=0, max_tokens=500,)\n",
    "\n",
    "# 체인 구성 (Chain Execution)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# 문서 포맷팅 (Formatting Docs)\n",
    "def format_docs(docs): # 청크 사이에 '\\n\\n'을 삽입\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "# 실행 (LLM에게 질의하기)\n",
    "response = chain.invoke({'context': (format_docs(docs)), 'question':query})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhHiFUyp7aNU"
   },
   "source": [
    "#### 07. 챗봇 UI 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4CQHey0P1ds"
   },
   "source": [
    "##### 필요한 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SINqrINo7c4s"
   },
   "outputs": [],
   "source": [
    "# streamlit 설치\n",
    "!pip install streamlit streamlit-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wx0C-W8iDA56"
   },
   "outputs": [],
   "source": [
    "# Colab 런타임에서 실행되는 Streamlit 앱을 외부에서 접근할 수 있도록 터널을 생성\n",
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BmKphN3--Y3q"
   },
   "outputs": [],
   "source": [
    "# 간단한 프로그램(app.py) 작성\n",
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "from streamlit_chat import message\n",
    "\n",
    "message(\"챗봇 메시지\")\n",
    "message(\"안녕하세요, 봇!\", is_user=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3uAF8Y1-y-J"
   },
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "PORT = 8501\n",
    "\n",
    "# Streamlit 앱을 백그라운드에서 실행(포트: 8501)\n",
    "process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", str(PORT), \"--server.enableCORS\", \"true\", \"--server.enableXsrfProtection\", \"false\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# ngrok 인증 토큰 설정 (ngrok 홈에서 계정생성 후, 받을 수 있음)\n",
    "ngrok.set_auth_token(\"2yM2FbBUyooXRCAtvsJfSnsM2Uw_4zXFe5a4ChPKYpSqcvK4X\")\n",
    "\n",
    "# ngrok 터널 생성\n",
    "public_url = ngrok.connect(PORT, bind_tls=True)\n",
    "print(f\"Streamlit 앱에 접속하려면 다음 링크를 클릭하세요: {public_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2p_lnlXM4tW"
   },
   "outputs": [],
   "source": [
    "# ngrok 터널링 종료를 위한 함수 (선택 사항)\n",
    "# Colab 세션이 끊기면 자동으로 종료되지만, 수동으로 종료하고 싶을 때 유용\n",
    "def kill_streamlit_ngrok():\n",
    "    process.terminate()\n",
    "    print(\"Streamlit 프로세스 종료...\")\n",
    "    ngrok.kill()\n",
    "    print(\"ngrok 터널 종료...\")\n",
    "\n",
    "# kill_streamlit_ngrok()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjslsMExUpRh"
   },
   "source": [
    "#### 예제: Gemini와 연동하는 챗봇 프로그램(chat.py) 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AcdIK7iMODHs"
   },
   "outputs": [],
   "source": [
    "%%writefile chat.py\n",
    "import streamlit as st\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# 타이틀 지정\n",
    "st.title(\"💬 Streamlit 챗봇\")\n",
    "\n",
    "# 초기화\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"assistant\", \"content\": \"무엇을 도와드릴까요?\"}\n",
    "    ]\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                             api_key=\"AIzaSyD9u7xQibC5jNJTz_qSEHBAORSUwWSe58w\",)\n",
    "\n",
    "def ask_llm(prompt):\n",
    "    response = llm.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 친절한 AI 챗봇입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}, ],\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.write(msg[\"content\"])\n",
    "\n",
    "if query := st.chat_input(\"질문을 입력하세요.\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "    st.chat_message(\"user\").write(query)\n",
    "    response = ask_llm(query)\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    st.chat_message(\"assistant\").write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dileiIeXV7xT"
   },
   "outputs": [],
   "source": [
    "# 실행 프로그램 작성\n",
    "from pyngrok import ngrok\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "PORT= \"8501\"\n",
    "\n",
    "# Streamlit 앱을 백그라운드에서 실행(포트: 8501)\n",
    "process = subprocess.Popen([\"streamlit\", \"run\", \"chat.py\", \"--server.port\", PORT, \"--server.enableCORS\", \"true\", \"--server.enableXsrfProtection\", \"false\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# ngrok 인증 토큰 설정(ngrok 홈에서 계정생성 후, 받을 수 있음)\n",
    "ngrok.set_auth_token(\"2yM2FbBUyooXRCAtvsJfSnsM2Uw_4zXFe5a4ChPKYpSqcvK4X\")\n",
    "\n",
    "# ngrok 터널 생성\n",
    "public_url = ngrok.connect(PORT, bind_tls=True)\n",
    "print(f\"Streamlit 앱 접속 링크: {public_url}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
